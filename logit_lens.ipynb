{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path visualizer function\n",
    "import sys\n",
    "sys.path.append('.')  # Add current directory to path\n",
    "\n",
    "from path_visualizer import PathVisualizer, visualize_generated_path\n",
    "\n",
    "# Test the path visualizer with the generated path\n",
    "example_path = \"19 28 19 left 18 left 17 down 27 down 37 left 36 up 26 up 16 up 6 left 5 down 15 down 25 down 35 down 45 right 46 right 47 right 48 down 58 right 59 down 69 down 79 down 89 left 88 down 98 left 97 up 87 left 86 left 85 left 84 left 83 left 82 down 92 left 91 up 81 left 80 up 70 right 71 up 61 left 60 up 50 up 40 up 30 right 31 up 21 right 22 down 32 down 42 left 41 down 51 right\"\n",
    "\n",
    "# Create visualizer\n",
    "visualizer = PathVisualizer()\n",
    "\n",
    "# Visualize the path (this will automatically skip the first 2 tokens as specified)\n",
    "fig = visualizer.visualize_path(\n",
    "    example_path,\n",
    "    title=\"Generated Path from GPT Model\",\n",
    "    save_path=\"generated_path_visualization.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"0 5 0\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 1 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration for training GPT on maze navigation data\n",
    "# Optimized for path learning task\n",
    "\n",
    "# I/O\n",
    "out_dir = 'out-maze-nav'\n",
    "eval_interval = 250\n",
    "log_interval = 10\n",
    "eval_iters = 100\n",
    "eval_only = False\n",
    "always_save_checkpoint = True\n",
    "\n",
    "# wandb logging\n",
    "wandb_log = False\n",
    "wandb_project = 'maze-nav'\n",
    "wandb_run_name = 'maze-nav-gpt'\n",
    "\n",
    "# data\n",
    "dataset = 'maze/maze_nav_data'  # This should match your maze data directory\n",
    "gradient_accumulation_steps = 1  # Reduced for smaller sequences\n",
    "batch_size = 32  # Reasonable batch size for path learning\n",
    "max_seq_len = 512  # Maximum sequence length for any path (no artificial limit)\n",
    "\n",
    "# model - smaller model suitable for maze navigation\n",
    "n_layer = 6   # Fewer layers for simpler task\n",
    "n_head = 6    # Fewer attention heads\n",
    "n_embd = 192  # Smaller embedding dimension\n",
    "dropout = 0.1 # Some dropout for regularization\n",
    "bias = False  # Cleaner model\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 3e-4  # Slightly higher learning rate\n",
    "max_iters = 5000      # Fewer iterations needed\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True\n",
    "warmup_iters = 100    # Quick warmup\n",
    "lr_decay_iters = 5000 # Match max_iters\n",
    "min_lr = 3e-5\n",
    "\n",
    "# DDP settings\n",
    "backend = 'nccl'\n",
    "\n",
    "# system\n",
    "device = 'cuda'\n",
    "# Note: dtype check is done in train.py\n",
    "dtype = 'bfloat16'  # will be validated in train.py\n",
    "compile = False \n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # init from a given GPT-2 model\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # Handle both string and integer tokens for maze navigation\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    \n",
    "    def encode(s):\n",
    "        if isinstance(s, str):\n",
    "            if s.isdigit():\n",
    "                # Handle numeric node IDs\n",
    "                return [stoi[s]] if s in stoi else [int(s)]\n",
    "            else:\n",
    "                # Handle direction tokens or other strings\n",
    "                tokens = s.split()\n",
    "                return [stoi[token] if token in stoi else int(token) if token.isdigit() else stoi.get(token, 0) for token in tokens]\n",
    "        elif isinstance(s, list):\n",
    "            # Handle list of tokens\n",
    "            return [stoi[str(token)] if str(token) in stoi else (int(token) if isinstance(token, str) and token.isdigit() else token) for token in s]\n",
    "        else:\n",
    "            return [stoi[str(s)] if str(s) in stoi else s]\n",
    "    \n",
    "    def decode(l):\n",
    "        if 'direction_tokens' in meta:\n",
    "            # Maze navigation specific decoding\n",
    "            result = []\n",
    "            for i in l:\n",
    "                token = itos.get(i, str(i))\n",
    "                result.append(token)\n",
    "            return ' '.join(result)\n",
    "        else:\n",
    "            # Standard string concatenation\n",
    "            return ''.join([itos.get(i, str(i)) for i in l])\n",
    "else:\n",
    "    # # ok let's assume gpt-2 encodings by default\n",
    "    # print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    # enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    # encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    # decode = lambda l: enc.decode(l)\n",
    "    raise ValueError(\"No meta.pkl found\")\n",
    "\n",
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "\n",
    "print(f\"Starting with: {start}\")\n",
    "start_ids = encode(start)\n",
    "print(f\"Encoded as: {start_ids}\")\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            generated_sequence = y[0].tolist()\n",
    "            decoded_output = decode(generated_sequence)\n",
    "            print(f\"Sample {k+1}:\")\n",
    "            print(f\"Raw tokens: {generated_sequence}\")\n",
    "            print(f\"Decoded: {decoded_output}\")\n",
    "            print('---------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the logit lens we have to define tokenizer\n",
    "import numpy as np\n",
    "\n",
    "class MazeNavTokenizer:\n",
    "    def __init__(self, encode, decode):\n",
    "        self._encode_fn = encode\n",
    "        self._decode_fn = decode\n",
    "\n",
    "    def encode(self, x):\n",
    "        return torch.tensor(self._encode_fn(x))\n",
    "\n",
    "    def __call__(self, x, **kwargs):\n",
    "        \"Return the token ids in ecco style\"\n",
    "        return self.encode(x)\n",
    "\n",
    "    def decode(self, x):\n",
    "        \"\"\"Decode tokens - handles both single tokens and lists\"\"\"\n",
    "        # Handle different input types\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            if x.dim() == 0:  # Single token (0-d tensor)\n",
    "                x = [x.item()]\n",
    "            else:\n",
    "                x = x.tolist()\n",
    "        elif isinstance(x, (np.ndarray, np.number)):\n",
    "            # Handle numpy arrays/scalars\n",
    "            if np.isscalar(x) or x.ndim == 0:\n",
    "                x = [int(x)]\n",
    "            else:\n",
    "                x = x.tolist()\n",
    "        elif not isinstance(x, list):\n",
    "            # Single token ID (int, float, etc.)\n",
    "            x = [x]\n",
    "        \n",
    "        # Convert any remaining numpy types in the list to regular Python types\n",
    "        clean_x = []\n",
    "        for item in x:\n",
    "            if hasattr(item, 'item'):  # numpy arrays and tensors\n",
    "                clean_x.append(item.item())\n",
    "            elif isinstance(item, (np.ndarray, np.number)):\n",
    "                clean_x.append(int(item))\n",
    "            else:\n",
    "                clean_x.append(item)\n",
    "        \n",
    "        # Now clean_x is always a list of hashable types, use the original decode function\n",
    "        return self._decode_fn(clean_x)\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return self.encode(tokens)\n",
    "    \n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        \"\"\"Convert token IDs to tokens\"\"\"\n",
    "        # Handle different input types\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.tolist()\n",
    "        elif not isinstance(ids, list):\n",
    "            ids = [ids]\n",
    "        \n",
    "        # For convert_ids_to_tokens, we need to return individual tokens\n",
    "        # not a joined string, so we'll decode each token separately\n",
    "        result = []\n",
    "        for token_id in ids:\n",
    "            # Handle numpy arrays and other numeric types\n",
    "            if hasattr(token_id, 'item'):  # numpy arrays and tensors\n",
    "                token_id = token_id.item()\n",
    "            elif isinstance(token_id, (np.ndarray, np.number)):\n",
    "                token_id = int(token_id)\n",
    "            \n",
    "            token_str = self._decode_fn([token_id])\n",
    "            result.append(token_str)\n",
    "        return result\n",
    "\n",
    "tokenizer = MazeNavTokenizer(encode, decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the tokenizer\n",
    "text = \"1 left 2\"\n",
    "print(f\"call: {tokenizer(text)}\")\n",
    "\n",
    "# Additional tests for the MazeNavTokenizer\n",
    "# Test 1: Encoding and decoding a simple string\n",
    "test_str = \"2 right 3\"\n",
    "encoded = tokenizer.encode(test_str)\n",
    "print(f\"Encoded '{test_str}': {encoded}\")\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Decoded back: {decoded}\")\n",
    "\n",
    "# Test 2: __call__ method\n",
    "call_result = tokenizer(test_str)\n",
    "print(f\"__call__('{test_str}'): {call_result}\")\n",
    "\n",
    "# Test 3: convert_tokens_to_ids and convert_ids_to_tokens\n",
    "tokens = \"1 left 2\"\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"convert_tokens_to_ids('{tokens}'): {ids}\")\n",
    "tokens_back = tokenizer.convert_ids_to_tokens(ids)\n",
    "print(f\"convert_ids_to_tokens({ids}): {tokens_back}\")\n",
    "\n",
    "# Test 4: Decoding a list of ids\n",
    "if hasattr(encoded, 'tolist'):\n",
    "    ids_list = encoded.tolist()\n",
    "else:\n",
    "    ids_list = list(encoded)\n",
    "decoded_list = tokenizer.decode(ids_list)\n",
    "print(f\"Decoded from list: {decoded_list}\")\n",
    "\n",
    "# Test 5: Edge case - empty string\n",
    "empty_encoded = tokenizer.encode(\"\")\n",
    "print(f\"Encoded empty string: {empty_encoded}\")\n",
    "empty_decoded = tokenizer.decode(empty_encoded)\n",
    "print(f\"Decoded empty string: {empty_decoded}\")\n",
    "\n",
    "# Test 6: Single token decoding (the problematic case)\n",
    "single_token = torch.tensor(1)\n",
    "print(f\"Single token tensor: {single_token}\")\n",
    "single_decoded = tokenizer.decode(single_token)\n",
    "print(f\"Single token decoded: {single_decoded}\")\n",
    "\n",
    "# Test 7: Single token ID as int\n",
    "single_id = 103\n",
    "single_decoded_int = tokenizer.decode(single_id)\n",
    "print(f\"Single token ID {single_id} decoded: {single_decoded_int}\")\n",
    "\n",
    "# Test 8: convert_ids_to_tokens with single token (what ecco uses)\n",
    "single_token_list = tokenizer.convert_ids_to_tokens([1])\n",
    "print(f\"convert_ids_to_tokens([1]): {single_token_list}\")\n",
    "single_token_only = tokenizer.convert_ids_to_tokens(1)\n",
    "print(f\"convert_ids_to_tokens(1): {single_token_only}\")\n",
    "\n",
    "# Test 9: numpy array handling (the new problematic case)\n",
    "import numpy as np\n",
    "numpy_token = np.array(103)\n",
    "print(f\"Numpy token: {numpy_token} (type: {type(numpy_token)})\")\n",
    "numpy_decoded = tokenizer.decode(numpy_token)\n",
    "print(f\"Numpy token decoded: '{numpy_decoded}'\")\n",
    "\n",
    "numpy_tokens_list = tokenizer.convert_ids_to_tokens([numpy_token])\n",
    "print(f\"convert_ids_to_tokens([numpy_token]): {numpy_tokens_list}\")\n",
    "\n",
    "# Test 10: numpy array in list\n",
    "numpy_list = [np.array(1), np.array(103), np.array(2)]\n",
    "print(f\"Numpy list: {numpy_list}\")\n",
    "numpy_list_decoded = tokenizer.decode(numpy_list)\n",
    "print(f\"Numpy list decoded: '{numpy_list_decoded}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the model structure is compatible with transformer_utils\n",
    "print(\"Model structure:\")\n",
    "print(\"model.base_model:\", type(model.base_model))\n",
    "print(\"model.base_model.h:\", type(model.base_model.h))\n",
    "\n",
    "# Check what modules the library can find\n",
    "print(\"\\nAll modules in base_model:\")\n",
    "for name, module in model.base_model.named_modules():\n",
    "    if 'norm' in name.lower() or 'ln' in name.lower():\n",
    "        print(f\"  {name}: {type(module)}\")\n",
    "\n",
    "# Let's also check the model structure more deeply        \n",
    "print(\"\\nDirect access tests:\")\n",
    "print(\"hasattr(model.base_model, 'ln_f'):\", hasattr(model.base_model, 'ln_f'))\n",
    "print(\"hasattr(model.base_model, 'final_layernorm'):\", hasattr(model.base_model, 'final_layernorm'))\n",
    "\n",
    "if hasattr(model.base_model, 'ln_f'):\n",
    "    print(\"model.base_model.ln_f:\", type(model.base_model.ln_f))\n",
    "if hasattr(model.base_model, 'final_layernorm'):\n",
    "    print(\"model.base_model.final_layernorm:\", type(model.base_model.final_layernorm))\n",
    "print(\"model.lm_head:\", type(model.lm_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.memmap(\"data/maze/maze_nav_data/val.bin\", dtype=np.uint16, mode=\"r\")\n",
    "\n",
    "# get all paths from the val\n",
    "data = []\n",
    "temp = []\n",
    "for d in val:\n",
    "    if d == 104:\n",
    "        data.append(temp)\n",
    "        temp = []\n",
    "    else:\n",
    "        temp.append(d)\n",
    "print(data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_utils.logit_lens import plot_logit_lens\n",
    "from transformer_utils.partial_forward import partial_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"0 left 0\"\n",
    "start_ids = tokenizer.encode(start)\n",
    "print(start_ids)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "y = model.generate(x, max_length=100, temperature=1, top_k=1)\n",
    "print(tokenizer.decode(y[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"0 5 0\").unsqueeze(0).to(device)\n",
    "input_ids = torch.tensor([0, 5, 0, 101, 91, 101, 96, 102]).unsqueeze(0).to(device)\n",
    "plot_logit_lens(model, tokenizer, input_ids, start_ix=0, end_ix=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ecco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ecco's LM with our fixed tokenizer\n",
    "# The tokenizer can now handle both single tokens and lists properly\n",
    "lm = ecco.LM(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name='maze-gpt', # A custom name for our maze navigation model\n",
    "    config={ # Config for ecco\n",
    "        'model_name': 'maze-gpt',\n",
    "        'type': 'causal',\n",
    "        'embedding': 'transformer.wte', # Path to embedding layer in our model\n",
    "        'activations': [\".*mlp.c_fc\"], # Regex to find MLP activation layers\n",
    "        'tokenizer_config': {}\n",
    "    },\n",
    "    verbose=True, # Show generation process\n",
    "    collect_activations_flag=True\n",
    ")\n",
    "\n",
    "# Test with a simple maze navigation input\n",
    "input_text = \"1 2 1\"\n",
    "input_ids = tokenizer.encode(input_text).unsqueeze(0)\n",
    "\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Input IDs: {input_ids}\")\n",
    "\n",
    "# Generate a few tokens\n",
    "output = lm.generate(input_ids, generate=5, attribution=['ig', 'saliency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.primary_attributions(attr_method='ig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.layer_predictions(position=4, layer=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.layer_predictions(position=3, layer=3) # TODO: check the indexing start with what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.rankings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"1 2 1 up 4 down 5\").unsqueeze(0)  \n",
    "print(input_ids)\n",
    "output = lm(input_ids)\n",
    "nmf_output = output.run_nmf(n_components=8)\n",
    "nmf_output.explore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

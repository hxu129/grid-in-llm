{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample from a trained model\n",
    "\"\"\"\n",
    "import os\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "import tiktoken\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
    "out_dir = 'out' # ignored if init_from is not 'resume'\n",
    "start = \"0 5 0\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
    "num_samples = 1 # number of samples to draw\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
    "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuration for training GPT on maze navigation data\n",
    "# Optimized for path learning task\n",
    "\n",
    "# I/O\n",
    "out_dir = 'out-maze-nav'\n",
    "eval_interval = 250\n",
    "log_interval = 10\n",
    "eval_iters = 100\n",
    "eval_only = False\n",
    "always_save_checkpoint = True\n",
    "\n",
    "# wandb logging\n",
    "wandb_log = False\n",
    "wandb_project = 'maze-nav'\n",
    "wandb_run_name = 'maze-nav-gpt'\n",
    "\n",
    "# data\n",
    "dataset = 'maze/maze_nav_data'  # This should match your maze data directory\n",
    "gradient_accumulation_steps = 1  # Reduced for smaller sequences\n",
    "batch_size = 32  # Reasonable batch size for path learning\n",
    "max_seq_len = 512  # Maximum sequence length for any path (no artificial limit)\n",
    "\n",
    "# model - smaller model suitable for maze navigation\n",
    "n_layer = 6   # Fewer layers for simpler task\n",
    "n_head = 6    # Fewer attention heads\n",
    "n_embd = 192  # Smaller embedding dimension\n",
    "dropout = 0.1 # Some dropout for regularization\n",
    "bias = False  # Cleaner model\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 3e-4  # Slightly higher learning rate\n",
    "max_iters = 5000      # Fewer iterations needed\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True\n",
    "warmup_iters = 100    # Quick warmup\n",
    "lr_decay_iters = 5000 # Match max_iters\n",
    "min_lr = 3e-5\n",
    "\n",
    "# DDP settings\n",
    "backend = 'nccl'\n",
    "\n",
    "# system\n",
    "device = 'cuda'\n",
    "# Note: dtype check is done in train.py\n",
    "dtype = 'bfloat16'  # will be validated in train.py\n",
    "compile = False \n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# model\n",
    "if init_from == 'resume':\n",
    "    # init from a model saved in a specific directory\n",
    "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
    "    model = GPT(gptconf)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    model.load_state_dict(state_dict)\n",
    "elif init_from.startswith('gpt2'):\n",
    "    # init from a given GPT-2 model\n",
    "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "if compile:\n",
    "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
    "\n",
    "# look for the meta pickle in case it is available in the dataset folder\n",
    "load_meta = False\n",
    "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
    "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
    "    load_meta = os.path.exists(meta_path)\n",
    "if load_meta:\n",
    "    print(f\"Loading meta from {meta_path}...\")\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    # Handle both string and integer tokens for maze navigation\n",
    "    stoi, itos = meta['stoi'], meta['itos']\n",
    "    \n",
    "    def encode(s):\n",
    "        if isinstance(s, str):\n",
    "            if s.isdigit():\n",
    "                # Handle numeric node IDs\n",
    "                return [stoi[s]] if s in stoi else [int(s)]\n",
    "            else:\n",
    "                # Handle direction tokens or other strings\n",
    "                tokens = s.split()\n",
    "                return [stoi[token] if token in stoi else int(token) if token.isdigit() else stoi.get(token, 0) for token in tokens]\n",
    "        elif isinstance(s, list):\n",
    "            # Handle list of tokens\n",
    "            return [stoi[str(token)] if str(token) in stoi else (int(token) if isinstance(token, str) and token.isdigit() else token) for token in s]\n",
    "        else:\n",
    "            return [stoi[str(s)] if str(s) in stoi else s]\n",
    "    \n",
    "    def decode(l):\n",
    "        if 'direction_tokens' in meta:\n",
    "            # Maze navigation specific decoding\n",
    "            result = []\n",
    "            for i in l:\n",
    "                token = itos.get(i, str(i))\n",
    "                result.append(token)\n",
    "            return ' '.join(result)\n",
    "        else:\n",
    "            # Standard string concatenation\n",
    "            return ''.join([itos.get(i, str(i)) for i in l])\n",
    "else:\n",
    "    # ok let's assume gpt-2 encodings by default\n",
    "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
    "    enc = tiktoken.get_encoding(\"gpt2\")\n",
    "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "    decode = lambda l: enc.decode(l)\n",
    "\n",
    "# encode the beginning of the prompt\n",
    "if start.startswith('FILE:'):\n",
    "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
    "        start = f.read()\n",
    "\n",
    "print(f\"Starting with: {start}\")\n",
    "start_ids = encode(start)\n",
    "print(f\"Encoded as: {start_ids}\")\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "# run generation\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            generated_sequence = y[0].tolist()\n",
    "            decoded_output = decode(generated_sequence)\n",
    "            print(f\"Sample {k+1}:\")\n",
    "            print(f\"Raw tokens: {generated_sequence}\")\n",
    "            print(f\"Decoded: {decoded_output}\")\n",
    "            print('---------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the logit lens we have to define tokenizer\n",
    "\n",
    "class MazeNavTokenizer:\n",
    "    def __init__(self, encode, decode):\n",
    "        self.encode = lambda x: torch.tensor(encode(x))\n",
    "        self.decode = lambda x: decode(x.tolist())\n",
    "\n",
    "tokenizer = MazeNavTokenizer(encode, decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the model structure is compatible with transformer_utils\n",
    "print(\"Model structure:\")\n",
    "print(\"model.base_model:\", type(model.base_model))\n",
    "print(\"model.base_model.h:\", type(model.base_model.h))\n",
    "\n",
    "# Check what modules the library can find\n",
    "print(\"\\nAll modules in base_model:\")\n",
    "for name, module in model.base_model.named_modules():\n",
    "    if 'norm' in name.lower() or 'ln' in name.lower():\n",
    "        print(f\"  {name}: {type(module)}\")\n",
    "\n",
    "# Let's also check the model structure more deeply        \n",
    "print(\"\\nDirect access tests:\")\n",
    "print(\"hasattr(model.base_model, 'ln_f'):\", hasattr(model.base_model, 'ln_f'))\n",
    "print(\"hasattr(model.base_model, 'final_layernorm'):\", hasattr(model.base_model, 'final_layernorm'))\n",
    "\n",
    "if hasattr(model.base_model, 'ln_f'):\n",
    "    print(\"model.base_model.ln_f:\", type(model.base_model.ln_f))\n",
    "if hasattr(model.base_model, 'final_layernorm'):\n",
    "    print(\"model.base_model.final_layernorm:\", type(model.base_model.final_layernorm))\n",
    "print(\"model.lm_head:\", type(model.lm_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_utils.logit_lens import plot_logit_lens\n",
    "from transformer_utils.partial_forward import partial_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"0 5 0\").unsqueeze(0).to(device)\n",
    "plot_logit_lens(model, tokenizer, input_ids, start_ix=0, end_ix=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
